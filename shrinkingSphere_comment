Spiego come ho implementato la sfera attorno al target
1) Funzione Reset
  - Ad ogni reset aumenta un contatore N
  - epsilon = epsilon0 * (1 - N/Episodi)
  
2) Funzione isArrived
  - Ad ogni step calcola la distanza tra il baricentro dell'UAV e il target
  - Compara questa distanza all'epsilon corrente
  - se (r - epsilon) <0 si attiva un flag isarrived
  
3) Funzione Reward
  - Ad ogni step controlla il valore del flag isarrived
  - Se isarrived == 1 il Reward viene devinito come la somma del reward fino all'istante dell'attivazione del flag + gli step rimanenti
  
4) Funzione isDone
  - Se isarrived == 1 viene terminato l'episodio.
  
Primo Problema : Nell'allenamento non so quanti episodi ci saranno, piuttosto so il numero di passi totali. Questo a causa del isDone (Episodi posssono essere molto meno lunghi 
del max_Steps_per_episode). Ho messo nell'ambiente da allenare un numero indicativo di episodi nell'allenamento--> non è assicurato che a fine training la sfera sia esattamente
un punto, ma da prove che ho fatto stamattina e ieri il valore Episodi sembra buono.

Secondo Problema : il contatore dell'ambiente di valutazione rimane molto indietro rispetto al valore dell'ambiente di training perchè viene richiamato dopo che ogni ambiente 
ha svolto 8192 passi di simulazione. Anche in questo caso so i passi ma non gli episodi, comunuque Episodi per l'ambiente di valutazione è più piccolo in modo da avere un andamento 
di espilon simile all'ambiente di training.

Osservazione : Dopo la definizione di epsilon ad ogni reset (self.sphere) c'è un print utile per vedere l'andamento del raggio. Essendo nel mio caso impiegati 6 core 
in parallelo, si vede che l'andamento del raggio è globalmente linearmente decrescente ma localmente (dipende sempre dal fatto gli episodi hanno lunghezza non predicibile causa isDone)
non è sempre decrescente. A lungo andare questo effetto mi pare comunque ininfluente nei risultati.

Nella cartella dei risultati ci sono due simulazioni con valori di Episodi diversi da quelli impostati negli esepmi svolti, dove ho notato che la sfera si riduceva troppo
velocemente nell'ambiente di training e troppo lentamente nell'ambiente di valutazione.

Nel frattempo ho controllato un buon numero di articoli della bibliografia dell'articolo e non ho trovato ancora i coefficienti, perciò vede i coefficienti che ho ipotizzato,
continuo la ricerca.

Agg. 20.57 09/03/2022
Ho trovato un numero indicativo di episodi e modifico il codice. Ho messo dei risultati di simulazioni svolte nel pomeriggio.
Agg. 09.13 10/03/2022
Questione sul Reward : negli allenamenti svolti vedo che il 'best_model' si ottiene sempre intorno ad un terzo dell'allenamento, questo secondo me perchè ad un terzo epsilon
è crica due terzi di quello iniziale, per il quadricottero è perciò più vicino, l'episodio è più breve e il guadagno (Ntot-elapsed_timesteps) è grande. Quando la sfera
si rimpicciolisce il quad passa più tempo fuori (anche pochi timesteps in più) e il guadagno finale risulta essere più piccolo rispetto a quello ottenuto con una sfera
di raggio maggiore. Prima di rifare il modello matematico dell'articolo metto un termine a moltiplicare (Ntot -elapsed_timesteps) del tipo (1 - epsilon/(2*epsilon0)) in modo
da garantire un guadagno maggiore anche negli stadi iniziali dell'allenamento se si raggiunge il target ma al tempo stesso premiare di più il raggiungimento 
della sfera quando questa è più piccola (all'istante 1 avrei 1/2*(Ntot-elapsed_timesteps) e quando la sfera è ad esempio 1/4 di quella iniziale 7/8*(Ntot-elapsed_timesteps)
Agg. 20.24 10/03/2022
Allego risultati di 25 simulazioni con l'agente allenato con l'accorgimento dell'aggiornamento precedente (SimulationResulats_1003). Sembra aver funzionato.
L'Agente è il PPO_Quad_3, il modello migliore (nella cartella EvalClbkLogs) è best_model_1003
